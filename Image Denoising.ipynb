{"cells":[{"metadata":{"id":"kHwulZ7OQQ3d","trusted":true},"cell_type":"code","source":"import numpy as np\nimport math\nfrom tensorflow import keras\nfrom keras.optimizers import RMSprop, Adagrad, Adam\nfrom keras.layers import Conv2D, Input, Dense, Dropout, MaxPool2D, UpSampling2D\nfrom keras.layers import Conv2DTranspose, BatchNormalization, add, LeakyReLU\nfrom keras.models import Model\nfrom keras.datasets import cifar10\nfrom keras.callbacks import ModelCheckpoint\nimport keras.layers as layers\nfrom keras.initializers import orthogonal\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"6OWD-POoQVy0","trusted":true},"cell_type":"code","source":"(cifar_train, _), (cifar_test, _) = cifar10.load_data()","execution_count":null,"outputs":[]},{"metadata":{"id":"6b9RATLJQWO-","trusted":true},"cell_type":"code","source":"size = 32\nchannel = 3\n# scaling input data\ncifar_train = cifar_train / 255\ncifar_test = cifar_test / 255\n\n# Adding noise mean = 0, std = 0.3\nnoise = 0.3\ncifar_train_noise = cifar_train + noise * np.random.normal(0, 0.3, size=cifar_train.shape) \ncifar_test_noise = cifar_test + noise * np.random.normal(0, 0.3, size=cifar_test.shape)\n\ncifar_train_noise = np.clip(cifar_train_noise, 0, 1)\ncifar_test_noise = np.clip(cifar_test_noise, 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"-lh2XNteQWZt","outputId":"7475bcf1-b363-407a-9be5-1450e522109f","trusted":true},"cell_type":"code","source":"# Visualize few training images with their noisy images\n\nrows = 2 # defining no. of rows in figure\ncols = 6# defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows*2)) # defining a figure \n\nfor i in range(rows):\n    for j in range(cols): \n        f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration\n        plt.imshow(cifar_train_noise[i*cols + j]) \n        plt.axis(\"off\")\n        \n    for j in range(cols): \n        f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration\n        plt.imshow(cifar_train[i*cols + j]) \n        plt.axis(\"off\")\n        \nf.suptitle(\"Sample Training Data\",fontsize=18)\nplt.savefig(\"Cifar-trian.png\")\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"huigSEy0QWXS","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"ePebb3o2QWUV","trusted":true},"cell_type":"code","source":"inputs = Input(shape=(size,size,channel))\ndef Model_1():\n  x = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n  x = BatchNormalization()(x)\n  x = MaxPool2D()(x)\n  x = Dropout(0.5)(x)\n  skip = Conv2D(32, 3, padding='same')(x) # skip connection for decoder\n  x = LeakyReLU()(skip)\n  x = BatchNormalization()(x)\n  x = MaxPool2D()(x)\n  x = Dropout(0.5)(x)\n  x = Conv2D(64, 3, activation='relu', padding='same')(x)\n  x = BatchNormalization()(x)\n  encoded = MaxPool2D()(x)\n\n  # Decoder\n  x = Conv2DTranspose(64, 3,activation='relu',strides=(2,2), padding='same')(encoded)\n  x = BatchNormalization()(x)\n  x = Dropout(0.5)(x)\n  x = Conv2DTranspose(32, 3, activation='relu',strides=(2,2), padding='same')(x)\n  x = BatchNormalization()(x)\n  x = Dropout(0.5)(x)\n  x = Conv2DTranspose(32, 3, padding='same')(x)\n  x = add([x,skip]) # adding skip connection\n  x = LeakyReLU()(x)\n  x = BatchNormalization()(x)\n  decoded = Conv2DTranspose(3, 3, activation='sigmoid',strides=(2,2), padding='same')(x)\n  return decoded\n\n\n\ndef Conv2DLayer(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n    prefix = f'block_{block_id}_'\n    x = layers.Conv2D(filters, kernel_size=kernel, strides=strides, padding=padding,\n                      kernel_initializer=kernel_init, name=prefix+'conv')(x)\n    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n    x = layers.Dropout(0.2, name=prefix+'drop')((x))\n    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n    return x\n\ndef Transpose_Conv2D(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n    prefix = f'block_{block_id}_'\n    x = layers.Conv2DTranspose(filters, kernel_size=kernel, strides=strides, padding=padding,\n                               kernel_initializer=kernel_init, name=prefix+'de-conv')(x)\n    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n    x = layers.Dropout(0.2, name=prefix+'drop')((x))\n    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n    return x\n\n\ndef Model_2():\n\n    conv1 = Conv2DLayer(inputs, 64, 3, strides=1, padding='same', block_id=1)\n    conv2 = Conv2DLayer(conv1, 64, 3, strides=2, padding='same', block_id=2)\n  \n    conv3 = Conv2DLayer(conv2, 128, 5, strides=2, padding='same', block_id=3)\n    \n    conv4 = Conv2DLayer(conv3, 128, 3, strides=1, padding='same', block_id=4)\n    conv5 = Conv2DLayer(conv4, 256, 5, strides=2, padding='same', block_id=5)\n    \n    conv6 = Conv2DLayer(conv5, 512, 3, strides=2, padding='same', block_id=6)\n    \n    deconv1 = Transpose_Conv2D(conv6, 512, 3, strides=2, padding='same', block_id=7)\n    \n    skip1 = layers.concatenate([deconv1, conv5], name='skip1')\n    conv7 = Conv2DLayer(skip1, 256, 3, strides=1, padding='same', block_id=8)\n    deconv2 = Transpose_Conv2D(conv7, 128, 3, strides=2, padding='same', block_id=9)\n    \n    skip2 = layers.concatenate([deconv2, conv3], name='skip2')\n    conv8 = Conv2DLayer(skip2, 128, 5, strides=1, padding='same', block_id=10)\n    deconv3 = Transpose_Conv2D(conv8, 64, 3, strides=2, padding='same', block_id=11)\n    \n    skip3 = layers.concatenate([deconv3, conv2], name='skip3')\n    conv9 = Conv2DLayer(skip3, 64, 5, strides=1, padding='same', block_id=12)\n    deconv4 = Transpose_Conv2D(conv9, 64, 3, strides=2, padding='same', block_id=13)\n    \n    skip4 = layers.concatenate([deconv4, conv1])\n    conv10 = layers.Conv2D(3, 3, strides=1, padding='same', activation='sigmoid',\n                       kernel_initializer=orthogonal(), name='final_conv')(skip4)\n\n    return conv10\n","execution_count":null,"outputs":[]},{"metadata":{"id":"brjUBCogQWMJ","trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('best_model_final.h5', verbose=1, save_best_only=True, save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"KEQkqUl5QWIa","outputId":"7f1afebb-384c-4a35-a76f-be30d0992ef1","trusted":true},"cell_type":"code","source":"# Encoder \ndecoder = Model_2()\n\n#decoder = Model_1()\nmodel = Model(inputs, decoder)\n\n\"\"\"\"\noptimizer = keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False\n)\n\noptimizer = keras.optimizers.Adagrad(\n    learning_rate=0.01,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07\n)\n\"\"\"\n#optimizer = Adam(lr=0.001,decay=0.001/80)\noptimizer = keras.optimizers.Adamax(lr=0.001)\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-2,\n    decay_steps=10000,\n    decay_rate=0.9)\n\n#optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n\n#model_opt = Adam(lr_schedule)\nmodel.compile(optimizer=optimizer,loss='mean_squared_error')\n#model.compile(loss='mean_squared_error', optimizer = RMSprop())\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"xPphVvgxQWGL","outputId":"9adc674c-5688-4d93-f746-7b24045ce70c","trusted":true},"cell_type":"code","source":"# Training\nepochs = 150\nbatch_size = 128\nhistory = model.fit(cifar_train_noise,\n                cifar_train,\n                epochs=epochs,\n                batch_size=batch_size,\n                shuffle=True,\n                validation_data=(cifar_test_noise, cifar_test),\n                callbacks=[checkpoint]\n               )","execution_count":null,"outputs":[]},{"metadata":{"id":"6CPaS5rcQWDi","trusted":true},"cell_type":"code","source":"# Defining Figure\nf = plt.figure(figsize=(10,7))\nf.add_subplot()\n\n#Adding Subplot\nplt.plot(history.epoch, history.history['loss'], label = \"loss\") # Loss curve for training set\nplt.plot(history.epoch, history.history['val_loss'], label = \"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\nplt.savefig(\"Loss_curve_cifar10.png\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mse(data_1, data_2):\n   return np.square(np.subtract(data_1, data_2)).mean()\n\ndef PSNR(data_1, data_2):\n    max_pixel = 1.0\n    return 10.0 * (1.0 / math.log(10)) * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true))))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"XwfyjldPQWCQ","trusted":true},"cell_type":"code","source":"import random\nimport tensorflow as tf\nfor i in range(20):\n  index = random.randint(0, 10000)\n  orig_images = cifar_test[index] \n  noised_original = cifar_test_noise[index]\n  noised_original_1 = noised_original.reshape(1,32,32,3)\n  test_desoided = model.predict(noised_original_1)\n  denoised_image  = test_desoided.reshape(32,32,3)\n\n  import matplotlib.pyplot as plt \n  %matplotlib inline\n  \n  fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(9,3))\n\n  axes[0].imshow(orig_images)\n  axes[0].set_title('Original image')\n\n  axes[1].imshow(noised_original)\n  mse1 = round(mse(orig_images, noised_original), 5)\n  psnr1 = round(tf.image.psnr(orig_images,noised_original,max_val=1.0).numpy(), 5)\n  axes[1].set_title('Noised image \\n MSE = {} \\n PSNR = {}'.format(mse1, psnr1))\n  mse2 = round(mse(denoised_image, orig_images), 5)\n  psnr2 = round(tf.image.psnr(orig_images,denoised_image,max_val=1.0).numpy(), 2) \n  axes[2].imshow(denoised_image)\n  axes[2].set_title('Denoised image \\n MSE = {} \\n PSNR = {}'.format(mse2, psnr2))\n  plt.savefig(\"image\"+str(i)+\".png\")\n  plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}